\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{fullpage}
\usepackage{url}

\begin{document}
\title{SIGVISA Processing}
\author{Dave Moore}
\date{}

This document describes the models algorithms used in SIGVISA's data processing. 

\section{Envelope Model}

\section{Observation Model}

\subsection{Model}
We model the observation at a three-component station by the following generative process. Each arriving phase $p$ generates an {\em abstract log-envelope} $\ell^{(p)}(t)$ according to the envelope model given above; the parameters of this envelope depend on the arriving amplitude, azimuth, angle of incidence, etc. (which in turn depend stochastically on the generating event). This abstract log-envelope is perturbed by an autoregressive process $w^{(p)}(t)$, given by 
\begin{align*}
w^{(p)}(t) &= \sum_{i=1}^{n_0} \alpha_i w^{(p)}(t-i) + \epsilon^{(p)}(t)\\
\epsilon^{(p)}(t) &\sim \mathcal{N}(0, \sigma_{(p)}^2)
\end{align*}
where $\alpha_1, \cdots, \alpha_{n_0}$ and $\sigma_{(p)}^2$ denote the weights and noise variance respectively of an order-$n_w$ autoregressive process (the ``wiggle process'' associated with this phase). The full abstract log-envelope $a^{(p)}(t)$ is thus given by
\[a^{(p)}(t) = \ell^{(p)}(t) + w^{(p)}(t)\]

A three-component station measures waveform energy along the east-west, north-south, and vertical axes. The energy of each arriving phase is projected onto those three components according to its arriving azimuth $\theta^{(p)}$ and angle of incidence $\phi^{(p)}$. If the abstract log-envelope of an arriving P wave is $a^{(p)}(t)$, then the projection onto component $k$ is given by $\log(\tau_k) + a^{(p)}$, where $\tau_k$ is a scaling factor given by
\begin{align*}
\tau^{(p)}_E &= |\tan \phi^{(p)} \cos\theta^{(p)}|\\
\tau^{(p)}_N &= |\tan \phi^{(p)} \sin\theta^{(p)}|\\
\tau^{(p)}_Z &=|\cos \phi^{(p)}|.
\end{align*}
For an S wave, {\bf I don't know what to do} since we know which direction the motion {\em isn't} in, but there are two ``transverse'' directions and I don't know how to distinguish between them. Maybe ignore this for the moment and pretend that S waves are like P waves (though they aren't,  of course). 

Finally, the observed envelope at component $k$ is generated by the sum of all projected arriving phase envelopes $\sum_{p\in \mathcal{P}} \exp(a^{(p)}(t))$, along with a component-specific {\em background noise process} $b^{(k)}(t)$, the log of which is again specified by an AR model with parameters $\beta^{(k)}_1, \cdots, \beta^{(k)}_{n_k}, \sigma_{(k)}^2$. So our final observed log-envelope is given by
\[h_k(t) = \log \left(\sum_{p\in\mathcal{P}} \exp(a^{(p)}(t)) + \exp(b^{(k)}(t))\right).\]

\subsection{Inference}

It turns out that the observation model described above is almost exactly a linear Gaussian model, suggesting a Kalman filter for state estimation and likelihood computation (recall that our main task is to compute the likelihood of the observed signal under a particular event hypothesis). Our state vector includes the current and previous values of the ``wiggle process'' for each currently arriving phase (up to the order of the AR model) along with the current and previous values of the noise processes for the three components. Considering for simplicity a single arriving phase $p$ and a single component noise process $k$, this is
\[\mathbf{x}(t) = \left(w^{(p)}(t), w^{(p)}(t-1), \cdots, w^{(p)}(t-n_0), b^{(k)}(t), b^{(k)}(t-1), \cdots, b^{(k)}(t-n_k)\right)^T.\]
The transition model is linear Gaussian, since the current value of each process is updated according to the AR model (which is just a linear combination of the previous values, plus Gaussian noise), and the previous values are just propogated along. That is, the transition model is given by
\[\mathbf{x}(t+1) = \mathbf{F}\mathbf{x}(t) + \mathbf{w}(t)\]
where $\mathbf{F}$ is a transition matrix which contains the AR weights and propagates the previous process values backwards in time:
\[\mathbf{F} = \left(\begin{array}{cccccccccc} \alpha_1 & \alpha_2 & \cdots & \alpha_{n_0-1} & \alpha_{n_0} & 0 & 0 & \cdots & 0 & 0 \\
1 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\
\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots\\
0 & 0 & \cdots & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cdots & 0 & 0 & \beta^{(k)}_1 & \beta^{(k)}_2 & \cdots & \beta^{(k)}_{n_k-1} & \beta^{(k)}_{n_k}\\
0 & 0 & \cdots & 0 & 0 & 1 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cdots & 0 & 0 & 0 & 1 & \cdots & 0 & 0 \\
\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots\\
0 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots & 1 & 0
\end{array}\right)\]
and $\mathbf{w}(t) \sim \mathcal{N}(0, \mathbf{Q})$ is a Gaussian noise vector with covariance $\mathbf{Q}$, where $\mathbf{Q}$ is a diagonal matrix whose only nonzero entries are $Q_{11} = \sigma_{(p)}^2$ and $Q_{(n_0+1)(n_0+1)} = \sigma_{(k)}^2$, i.e. the variances of the autoregressive processes being modeled.

Unfortunately the observation model is {\em not} linear since we are modeling the envelopes in the log scale. Now if we let $\mathcal{P}$ be the set of currently arriving phases (assumed above to contain just a single element $p$, but now allowed to be arbitrary), we have
\[\mathbf{z}(t) = \mathbf{h}(\mathbf{x}(t)) + \mathbf{v}(t)\]
where 
\[\mathbf{h}(\mathbf{x}(t)) = \left(\begin{array}{c}\log\left(\sum_{p\in\mathcal{P}} \tau^{(p)}_E \exp\left(a^{(p)}(t)\right) + \exp(b^{(E)}(t)) \right)\\
\log\left(\sum_{p\in\mathcal{P}} \tau^{(p)}_N \exp\left(a^{(p)}(t)\right) + \exp(b^{(N)}(t)) \right)\\
\log\left(\sum_{p\in\mathcal{P}} \tau^{(p)}_Z \exp\left(a^{(p)}(t)\right) + \exp(b^{(Z)}(t)) \right)
\end{array}\right)\]
is our (nonlinear) measurement function and 
\[\mathbf{v}(t) = \mathbf{0},\]
i.e. we assume no additional measurement noise (recall that we already include AR noise processes for each component as part of our state vector). ({\bf is this tenable? do we need some noise just for numerical stability?}) Since $\mathbf{h}$ is nonlinear we cannot use a traditional Kalman filter to compue the likelihood. Since $\mathbf{h}$ is differentiable we could linearize it around the current state estimate $\mathbf{\hat{x}(t)}$, i.e. an extended Kalman filter (EKF), but instead we opt to use an unscented Kalman filter (UKF). While the EKF uses an approximation of $\mathbf{h}$ to compute an exact (Gaussian) posterior distribution, the UKF uses the true $\mathbf{h}$ applied to a set of deterministic sample points to compute an approximation of the posterior distribution. 

Further details on the UKF can be found on Wikipedia (and at \url{stomach.v2.nl/Docs/TechPubs/Tracking_and_AR/wan01unscented.pdf}), but for clarity we derive here the likelihood function $p(\mathbf{z}_1, \cdots, \mathbf{z}_t)$. The chain rule allows us to write this as a recursive update:
\[\log p(\mathbf{z}_1, \cdots, \mathbf{z}_t) = \log p(\mathbf{z}_t | \mathbf{z}_1, \cdots, \mathbf{z}_{T-1}) + \log p(\mathbf{z}_1, \cdots, \mathbf{z}_{t-1})\]
and so we just need to derive the update factor
\begin{align*}
p(\mathbf{z}_t | \mathbf{z}_1, \cdots, \mathbf{z}_{t-1}) &= \int p(\mathbf{z}_t | \mathbf{x}_t) p(\mathbf{x}_t | \mathbf{z}_1, \cdots, \mathbf{z}_{t-1})d\mathbf{x}_t
\end{align*} 
where \[p(\mathbf{x}_t | \mathbf{z}_1, \cdots, \mathbf{z}_{t-1}) \sim \mathcal{N}\left(\hat{\mathbf{x}}_{t|t-1}, \mathbf{P}_{t|t-1}\right)\]
 is the Gaussian distribution predicted by the Kalman filter. In a standard KF the observation model $p(\mathbf{z}_t | \mathbf{x}_t)$ is also linear Gaussian:
\[p(\mathbf{z}_t | \mathbf{x}_t) \sim \mathcal{N}\left( \mathbf{H}\mathbf{x}_t, \mathbf{R} \right)\]
and the integral gives a Gaussian density \[p(\mathbf{z}_t | \mathbf{z}_1, \cdots, \mathbf{z}_{t-1}) \sim \mathcal{N}\left( \mathbf{H} \hat{\mathbf{x}}_{t|t-1}, \mathbf{R} + \mathbf{H}\mathbf{P}_{t|t-1}\mathbf{H}^T\right)\]
(see identity 35 in \url{http://userpage.fu-berlin.de/mtoussai/notes/gaussians.pdf}), so our update factor is just a Gaussian likelihood. Now in our case, we have $\mathbf{R}=\mathbf{0}$ since we assume no additional measurement noise, but more importantly our measurements are no longer linear. In an EKF we would just take $\mathbf{H}$ to be the linear approximation of $\mathbf{h}$ around $\hat{\mathbf{x}}_{t|t-1}$.

Note that what we just derived is just the ``innovation covariance'' $\mathbf{S}$ from wikipedia. 


problems:

- the idea of an abstract wiggle process means that wiggles in all channels have to look almost exactly the same. is this the case in real wiggles? I'm guessing not... so I either focus on real data, or add iid observation noise... but that sort of defeats the point of AR noise in the first place. 
-- I could also add separate wiggle processes for each channel, but then there'd be no correlation between the channels at all (except the envelope, which I guess counts for something...)
-- separate wiggle processes is actually the only way I can think of to legitimately learn a wiggle model (without crazy Kalman stuff...)

-- another thought: instead of having an elephant model, just learn a separate AR noise variance for the first few seconds of the signal...
 
-- steps:
-- when this is working: integrate into fit_envelope!
-- hook in Min's code to learn AR noise params
-- once this is all good and working, go back to (iteratively) learning signal AR params
-- keep in mind the whole time that we will need to do this for lots of frequency bands...
- once this is all working, go back and try to figure out how P/S projection coeffs should work (learn a model...)


- mystery segfault/stack corruption when I remove the canary


issues w/ integration:
- need to decide when to have obs noise and when not to (or, whether to just have indep wiggle models for each channel...)

problems:
- costs are barely changing?
- fix horiz_avg ( or just work with bhz...)


again, the optimizer isn't even TRYING
further, there's an issue with big negative costs (enormous lls) for some params ... is this OK?

let's try what I think are the ``true'' params, compare to what the optimizer finds

if we have an arrival at zero height, then suddenly we get twice the variance to explain the same noise signal? 
if we think of adding things in real space, we have an exp-AR noise process, plus a peak-height-'e' exponentially-decaying envelope being multiplied by an exp-AR process.
under this model a logamp-0 arrival will almost always make a signal more plausible (which makes a weird sort of sense...)
this will be solved by a prior on event sizes, and then on the amplitudes created by event sizes...

fixed the not adding zero events, but haven't fixed the outcome.

I don't believe that my sigma is correct ... the noise residuals I'm seeing are ALL like .01, but my stddev is apparently .07? (var .005)
the stddev of residuals from the first 1000 datapoints is 0.0233

so maybe the noise variance just isn't representative?

even with low noise variance, the ``best params'' are getting lower costs (higher ll) than the ``true params''. 
the variances shouldn't be too different: they both have mixtures of the same AR processes (but how do variances work with the mixed processes?)
next step: extract residuals and variances from the lls logs for ``true'' vs. ``best''

one issue: we start by predicting the mean, and charging enormously in ll for any deviation from it. we should probably just fix the initial state to the first observation?

also:
``true'' has high residuals throughout the arrivals. ``best'' has a couple of big ones but then it adjusts, to having mostly smaller residuals.
``true'' has variances matchine the shape of the signal. ``best'' has big variances at the time of each arrival, but then mostly small again.

I don't understand how ``best'' doesn't have GIANT residuals. 

it makes sense that the variances should follow the envelope outlines: the bigger the envelope, the greater the ability of the wiggle process to influence the outcome. 
I suppose it's possible that high variances could cause lower likelihoods, so this would be effectively saying that we don't *need* the wiggle process to understand the variation taking place.

the problem:  by effectively not having an event, we are forcing the noise process to explain the arrival. this is really expensive, once. (at the time of the arrival). but just half a second later, the noise process has jumped up, and is now doing fine. and it now constitutes a better explanation than the actual wiggle process, because the variance on the wiggle process is way too high?

solutions:
a) lower the variance on the wiggle process.
b) run the noise process in the original scale, not the log scale. the chance of getting a series of jumps up to the level of a phase arrival, just by random chance, ought to be essentially zero.

the residuals are still waaay off. and now we pay more for them.
I think the issue is that we're not properly tracking the wiggle process -- or maybe we don't have the right params for the wiggle process. 
the Kalman gain controls how much of the adjustment goes into which hidden state


I should build proper debugging tools for this, since I imagine it will come up a lot.

need a DEBUG flag which causes dumping to a new dir everytime likelihood is called.
in this dir are files for the residual, output variance, predicted mean, and Kalman gain.
then I have a matlab script which, given the dir name, loads these files and plots stuff.

okay, now I have the gain and state logs

the major issue is the residuals. why are residuals larger under the ``true'' params than under the ``best'' params? it seems to be yo-yoing back and forth constantly
meanwhile, the ``best'' params should have constantly negative residuals during the time of the signal spike, since the arrival energy has been decaying. 
yet they don't! 



just to be absolutely sure about the stats, maybe I should go ahead and write a subtract-envelope function. what I want:
- give it a trace, and a set of envelope params
- return the trace with that envelope subtracted
- be able to use this to learn AR wiggle models. (so I should be ignoring the noise level... how does this work? exponentiate the signal,and the envelope. subtract the envelope, then take the log. 

the confusing thing here is that if I exponentiate a nonzero-mean AR noise process, then subtract the mean, then take the log, this will stretch the process in a weird way. 
this isn't an issue for the noise process param estimates, because I'm subtracting the mean in the log domain
but even focusing solely on the wiggle process, say I have exp(template + wiggleAR) and exp(template). to get wiggleAR I would actually just subtract the log-template from the log-obs. this is the right thing as long as the signal level is actually several orders of magnitude above the noise. (e.g. from looking at the envelope it should be clear that the noise could *never* reach up to the signal level). 


my assumption had been that the signal is exp(mean + noiseAR) + exp(template + wiggleAR) 



) 

\end{document}